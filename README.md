# Simple Captioner

  Changes:
  
- Modified to run offline using local models defined in app.py
- Defined models/paths will appear in the model selection list.
- Original model selections are hidden.
- Flash Attention 2 is enabled by default.
- Quantization is disabled by default.
- Increased default max tokens.
- Changed default prompt.

## Notes

I am not planning to build on, or update this repository.

It's just a custom fork for my own convenience.

All credit goes to Olli S.


---

## Windows Installation (PyTorch 2.10 + CUDA 13.0 + Python 3.12 + Flash Attention 2.8.3)

  Via Powershell:

1. **Clone the repository**:

   ```bash
   git clone https://github.com/Highstate/simple-captioner.git SimpleCaptioner
   cd SimpleCaptioner

2. Create and activate virtual environment:

    ```bash
    python -m venv venv
    Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
    venv/Scripts/Activate.ps1

3. Install Torch

    ```bash
    pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130
    
4. Install Flash Attention

   Download the Flash Attention wheel from (https://github.com/wildminder/AI-windows-whl?tab=readme-ov-file) and move it to your SimpleCaptioner folder

    ```bash
    pip install ".\flash_attn-2.8.3+cu130torch2.10-cp312-cp312-win_amd64.whl"

5. Install Gradio

    ```bash
    pip install gradio==5.31.0

6. Install dependencies

    ```bash
    pip install -r requirements.txt

7. Install Triton-Windows

    ```bash
    pip install triton-windows

8. Run SimpleCaptioner

    Use run_app.bat or from the command line:
   
    ```bash
    python app.py

---

# Simple Captioner

A minimal media captioning tool powered by **[Qwen2.5/3 VL Instruct](https://huggingface.co/Qwen/)** from Alibaba Group.

This tool uses a Gradio UI to batch process folders of **images and videos** and generate descriptive captions.

Written by [Olli S.](https://github.com/o-l-l-i)

---

![Splash image](/images/screenshot.png)

## ‚ú® Features

Version 1.0.1

- ‚úÖ Uses `Qwen2.5/3 VL Instruct` for high-quality understanding
- ‚úÖ Support for:
  - Qwen/Qwen3-VL-4B-Instruct
  - Qwen/Qwen3-VL-8B-Instruct
  - Qwen/Qwen2.5-VL-3B-Instruct
  - Qwen/Qwen2.5-VL-7B-Instruct
- ‚úÖ Flash attention 2 support (with toggle)
- ‚úÖ Quantization via BitsAndBytes (None / 8-bit / 4-bit)
- ‚úÖ Caption multiple images or videos from a selected folder
- ‚úÖ Sub-folder support
- ‚úÖ Supports prompt customization
- ‚úÖ "Summary Mode" and "One-Sentence Mode" options for different caption styles
- ‚úÖ Can skip already-captioned images
- ‚úÖ Image previews with real-time progress
- ‚úÖ Abort long runs safely

---

## Requirements

- Python 3.9+
- A modern NVIDIA GPU with CUDA (tested on Ampere and newer)
- ~16GB VRAM recommended for smooth operation

---

## Setup Instructions

1. **Clone the repository**:

   ```bash
   git clone https://github.com/o-l-l-i/simple-captioner.git
   cd simple-captioner

2. **Create a virtual environment (optional but recommended)**:

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate

3. **Install the dependencies**:

    ```bash
    pip install -r requirements.txt

4. **Install Torch with GPU support**:
   - You have to install GPU compatible Torch yourself, get it from here:
   - [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
   - Copy the "Run this Command" string from the page after selecting correct version.
     - i.e. if you have Cuda 12.8, select that option. (Windows, Pip, Python, CUDA 12.8.)

    ```bash
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

5. **Install Triton**:

    On Windows, install [woctordho's Triton fork for Windows](https://github.com/woct0rdho/triton-windows)

    ```bash
    pip install triton
    pip install triton-windows # On Windows use this

6. **Run the app**:

    ```bash
    python app.py

7. **To run this app later**:

    - When you need to return back to use this, the virtual environment (venv) needs to be activated again.
    - Use/modify the included start up scripts.

    **Windows**:
    - run_app.bat

    ```bash
    @echo off
    call venv\Scripts\activate
    python app.py
    ```

    **Linux/macOS**:
    - run_app.sh

    ```bash
    #!/bin/bash
    source venv/bin/activate
    python app.py
    ```

    Make it executable:
    ```bash
    chmod +x run_app.sh
    ```

## Model Files

When you run the app for the first time, the model (Qwen/Qwen2.5-VL-7B-Instruct) is automatically downloaded from Hugging Face. This download is cached locally, so subsequent runs are much faster and offline-compatible.

By default, Hugging Face stores downloaded models in:


```bash
Linux/macOS: ~/.cache/huggingface/

Windows: C:\Users\<YourUsername>\.cache\huggingface\
```

You can inspect, manage, or clear this cache manually, or change the location by setting the HF_HOME environment variable:


```bash
export HF_HOME=/custom/path/to/huggingface
# On Windows: set HF_HOME=E:\huggingface_cache
```

This is useful if you're working with limited disk space or want to centralize model caches across multiple projects.

---

## Video Support Note
To enable video processing, make sure qwen-vl-utils is installed.
On Linux:

```bash
pip install qwen-vl-utils[decord]==0.0.8

```

```bash
On other platforms (Windows/macOS):
pip install qwen-vl-utils
```

This will fall back to using torchvision for video loading if decord does not work, which is slower.
For better performance, [you can try to install decord from source](https://github.com/dmlc/decord)

## Usage Notes
- Place your images in a folder (recursively scanned, subfolders are supported.)
- Text files with the same name (e.g. image1.jpg ‚Üí image1.txt) are created alongside the images.
- Use the ‚ÄúSkip already captioned‚Äù checkbox to avoid reprocessing.
- Captions can be styled with prompt modifiers or sentence-length constraints.

---

## Customization

- Prompt handling is adjustable with toggles.
- Modify the base prompt or model behavior in generate_caption() inside the code.
- Want more control over output format? Adjust the file writing or UI code.

---

## Troubleshooting

- Make sure you‚Äôre using a CUDA-compatible GPU.
- On Windows you have to install GPU compatible Torch yourself, get it from here:
  - [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
  - Select a Torch version which matches your CUDA version.
- If VRAM usage is too high, reduce max_tokens. This is only tested on 3090 and 5090, but I did monitor the VRAM usage.

---

## Versions

- **1.0.1 - 2025-10-15**
  - Model dropdown with multiple model support.
  - Quantization (None / 8-bit / 4-bit.)
  - Attention implementation toggle (flash attention 2 supported) + auto-fallback to `eager`
  - Model is no longer loaded at import; loads via UI or on app UI start.
  - Defaults to Qwen/Qwen3-VL-8B-Instruct, this can be memory intensive, so use quantization or 4B model.
  - Improved VRAM cleanup.

- **1.0.0**
  - Initial release.
  - Qwen/Qwen2.5-VL-7B-Instruct support for image and video captioning.

---

## Early Development Notice

This project is currently in a very early phase of development. While it aims to provide useful image and video captioning capabilities, you may encounter bugs, unexpected behavior, or incomplete features.

If you run into any issues:

- Please check the console or logs for error messages.
- Try to use supported media formats as listed.
- Feel free to report problems or request features via the project‚Äôs GitHub Issues page.

---

## License & Usage Terms

Copyright (c) 2025 Olli Sorjonen

This project is source-available, but not open-source under a standard open-source license, and not freeware.
You may use and experiment with it freely, and any results you create with it are yours to use however you like.

However:

Redistribution, resale, rebranding, or claiming authorship of this code or extension is strictly prohibited without explicit written permission.

Use at your own risk. No warranties or guarantees are provided.

The only official repository for this project is: üëâ https://github.com/o-l-l-i/simple-captioner

---

## Author

Created by [@o-l-l-i](https://github.com/o-l-l-i)
